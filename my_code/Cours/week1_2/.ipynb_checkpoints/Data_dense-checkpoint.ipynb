{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T15:04:06.745391Z",
     "start_time": "2019-10-30T15:04:06.579573Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Data Dense\n",
    "    => eliminate rare fields and values\n",
    "\"\"\"\n",
    "# importing libs\n",
    "# nombre de partition (cloisons) par default dans le SC = Le nombre de Workers\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "# spark context avec 4 workers\n",
    "if sc is not None:\n",
    "    sc.stop()\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "#import sql context\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "sqlContext = SQLContext(sc)\n",
    "# dataframes\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, BinaryType\n",
    "\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "def unpackArray(x,data_type=np.float16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)\n",
    "\n",
    "def count_nan(V):\n",
    "    \"\"\"\n",
    "    count number of NaN in a bytearray\n",
    "    \n",
    "    :param V: a bytearray\n",
    "    \"\"\"\n",
    "    A = unpackArray(V,data_type=np.float16)\n",
    "    return int(sum(np.isnan(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T09:29:25.412876Z",
     "start_time": "2019-10-30T09:29:24.986612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ucsddse230/work/my_code/cours/week1_2/data\n",
      "mobydick  people2.parquet  people.parquet  Weather\n"
     ]
    }
   ],
   "source": [
    "# definition repertoire ou on va mettre les fichiers AWS (dans la VM)\n",
    "%cd /home/ucsddse230/work/my_code/cours/week1_2/data\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T09:29:34.108304Z",
     "start_time": "2019-10-30T09:29:33.690563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘Weather’: File exists\n",
      "/home/ucsddse230/work/my_code/cours/week1_2/data/Weather\n"
     ]
    }
   ],
   "source": [
    "# creation du ss rep Weather s'il n'existe pas, in se place dedans\n",
    "!mkdir Weather\n",
    "%cd Weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:07:46.776935Z",
     "start_time": "2019-10-30T08:07:35.047236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting awscli\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/18/3e360dd5503172f49c758e221aae5809d7ecf035874b252f340cf688d54a/awscli-1.16.269-py2.py3-none-any.whl (2.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.4MB 264kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10 (from awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama<0.4.2,>=0.2.5; python_version != \"2.6\" and python_version != \"3.3\" (from awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<5.2,>=3.10; python_version != \"2.6\" and python_version != \"3.3\" in /opt/conda/lib/python3.6/site-packages (from awscli)\n",
      "Collecting botocore==1.13.5 (from awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/d8/f1fb1d6afe096fc7786187bea1a92fc7ebfec240ebd4d9ae8a36fc632e9a/botocore-1.13.5-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.3MB 106kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<=3.5.0,>=3.1.2 (from awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.13.5->awscli)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from botocore==1.13.5->awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.13.5->awscli)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<=3.5.0,>=3.1.2->awscli)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.13.5->awscli)\n",
      "Installing collected packages: docutils, colorama, jmespath, botocore, s3transfer, pyasn1, rsa, awscli\n",
      "Successfully installed awscli-1.16.269 botocore-1.13.5 colorama-0.4.1 docutils-0.15.2 jmespath-0.9.4 pyasn1-0.4.7 rsa-3.4.2 s3transfer-0.2.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install aws\n",
    "#!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-30T15:51:41.216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Access Key ID [None]: "
     ]
    }
   ],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T15:51:35.229847Z",
     "start_time": "2019-10-30T15:51:34.200749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to locate credentials. You can configure credentials by running \"aws configure\".\r\n"
     ]
    }
   ],
   "source": [
    "# check les fichiers présent sur ce noeud\n",
    "!aws s3 ls s3://dse-weather/ALL.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T09:29:43.996085Z",
     "start_time": "2019-10-30T09:29:43.578660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY.parquet  NY.tgz\r\n"
     ]
    }
   ],
   "source": [
    "#### O nlaisse tomber aws pour le moment on essaie de faire fonctionner ça avec les parquet de l'exercice dataframe_operatiosn\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T14:38:00.428286Z",
     "start_time": "2019-10-30T14:37:59.423827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory :  /home/ucsddse230/work/my_code/cours/week1_2/data/Weather\n",
      "SELECT measurement, count(measurement) as Count FROM parquet.`/home/ucsddse230/work/my_code/cours/week1_2/data/Weather/NY.parquet` GROUP BY measurement\n",
      "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
      "Wall time: 997 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import avec Query SQL : on ne prend pas tout le contenu du parquet\n",
    "weather_parquet_dir = os.getcwd()\n",
    "print('Directory : ', weather_parquet_dir)\n",
    "\n",
    "Query = \"SELECT measurement, count(measurement) as Count FROM parquet.`%s/NY.parquet` GROUP BY measurement\" %(weather_parquet_dir)\n",
    "print(Query)\n",
    "counts_pdf = sqlContext.sql(Query).toPandas()\n",
    "#toPandas() => converts rows to dataframe\n",
    "\n",
    "counts_pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T14:39:40.518382Z",
     "start_time": "2019-10-30T14:39:40.512742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(Station,StringType,true),\n",
       " StructField(Measurement,StringType,true),\n",
       " StructField(Year,IntegerType,true),\n",
       " StructField(Values,BinaryType,true)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créatiob schema\n",
    "schema = StructType([StructField(\"Station\",     StringType(), True),\n",
    "                     StructField(\"Measurement\", StringType(), True),\n",
    "                     StructField(\"Year\",        IntegerType(),True),\n",
    "                     StructField(\"Values\",      BinaryType(),True)\n",
    "                    ])\n",
    "list(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T14:39:45.133900Z",
     "start_time": "2019-10-30T14:39:44.992777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM parquet.`/home/ucsddse230/work/my_code/cours/week1_2/data/Weather/NY.parquet`\n",
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Query2 = \"SELECT * FROM parquet.`%s/NY.parquet`\" %(weather_parquet_dir)\n",
    "print(Query2)\n",
    "DFWeather = sqlContext.sql(Query2)\n",
    "DFWeather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T14:39:48.955184Z",
     "start_time": "2019-10-30T14:39:48.465829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              Values|\n",
      "+--------------------+\n",
      "|[00 00 00 00 00 0...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFWeather.select('Values').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T15:04:13.042232Z",
     "start_time": "2019-10-30T15:04:13.032300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-db757a7b0254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_nan_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCount_nan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'udf' is not defined"
     ]
    }
   ],
   "source": [
    "count_nan_udf = udf(Count_nan,IntegerType())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
