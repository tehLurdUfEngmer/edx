{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:41:32.103121Z",
     "start_time": "2019-10-28T16:41:28.630297Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Dataframe : Un sous objet RDD restreint (les types sont restreint => optimisation)\n",
    "    => 2 D : lignes / colonne\n",
    "        -> chaque colonne à un type définit\n",
    "        -> chaque ligne contient un enregistrement\n",
    "    => similaire aux dataframes de R ou pandas\n",
    "\"\"\"\n",
    "# nombre de partition (cloisons) par default dans le SC = Le nombre de Workers\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "# spark context avec 4 workers\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "#import sql context\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# dataframes\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:37:45.270732Z",
     "start_time": "2019-10-28T14:37:45.192562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=28, name='John'),\n",
       " Row(age=36, name='Dwayne'),\n",
       " Row(age=21, name='Adam'),\n",
       " Row(age=40, name='Andree')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Creation of a dataframe using the Row object\n",
    "\"\"\"\n",
    "# creation of a rdd of elements of type row : \n",
    "some_rdd = sc.parallelize([Row(name=u\"John\", age=28),\n",
    "                            Row(name=u\"Dwayne\", age=36),\n",
    "                            Row(name=u\"Adam\", age=21),\n",
    "                            Row(name=u\"Andree\", age=40)])\n",
    "\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:37:47.193800Z",
     "start_time": "2019-10-28T14:37:46.696793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transform into RDD :\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "#schema : name and type of each columns\n",
    "some_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:37:48.559047Z",
     "start_time": "2019-10-28T14:37:48.409292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of RDD :  <class 'pyspark.rdd.RDD'> \n",
      " Type of DataFrame :  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "RDD :  [Row(age=28, name='John'), Row(age=36, name='Dwayne'), Row(age=21, name='Adam'), Row(age=40, name='Andree')]\n",
      "RDD :  [Row(age=28, name='John'), Row(age=36, name='Dwayne'), Row(age=21, name='Adam'), Row(age=40, name='Andree')]\n"
     ]
    }
   ],
   "source": [
    "# collecting a RDD and a DataFrames gives the same output:\n",
    "print('Type of RDD : ',type(some_rdd),'\\n Type of DataFrame : ', type(some_df))\n",
    "print('RDD : ',some_rdd.collect())\n",
    "print('RDD : ',some_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T15:33:26.978216Z",
     "start_time": "2019-10-28T15:33:26.653425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- personName: string (nullable = false)\n",
      " |-- personAge: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Creation of a dataframe using a schema and using t-uples RDD\n",
    "\"\"\"\n",
    "\n",
    "rdd0 = sc.parallelize([('Pedro',24),('Hernesto',41),('Maria',29),('Javier',33),('Alan',49),('Luciana',21),])\n",
    "# definition of the schema : => StrucField(nom, type, nullable)\n",
    "schema = StructType([StructField('personName',StringType(),False),\n",
    "                  StructField('personAge',IntegerType(),False)])\n",
    "#creation of the DataFrame :\n",
    "df0 = sqlContext.createDataFrame(rdd0,schema)\n",
    "df0.printSchema()\n",
    "\n",
    "# writing DF into parquet file :\n",
    "df0.write.parquet('data/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T15:33:50.531802Z",
     "start_time": "2019-10-28T15:33:49.940955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|personName|personAge|\n",
      "+----------+---------+\n",
      "|  Hernesto|       41|\n",
      "|     Maria|       29|\n",
      "|      Alan|       49|\n",
      "|   Luciana|       21|\n",
      "|    Javier|       33|\n",
      "|     Pedro|       24|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|personName|personAge|\n",
      "+----------+---------+\n",
      "|  Hernesto|       41|\n",
      "|     Maria|       29|\n",
      "|      Alan|       49|\n",
      "|   Luciana|       21|\n",
      "|    Javier|       33|\n",
      "|     Pedro|       24|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Loading dataframes from Disk :\n",
    "    => Parquet\n",
    "    => JSON\n",
    "    => CSV\n",
    "\"\"\"\n",
    "parquetFileDir = 'data/people.parquet'\n",
    "df1 = sqlContext.read.load(parquetFileDir)\n",
    "df1.show()\n",
    "# show df using select method\n",
    "df1_select = df1.select('personName','personAge')\n",
    "df1_select.show()\n",
    "df1_select.write.parquet('data/people2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:39:32.641595Z",
     "start_time": "2019-10-28T16:39:19.234396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ucsddse230/work/PA1_collinear_points/pa1/PySparkNoteBook\n",
      "/home/ucsddse230/work/PA1_collinear_points/Data\n",
      "making /home/ucsddse230/work/PA1_collinear_points/Data\n",
      "/home/ucsddse230/work/PA1_collinear_points/Data/Weather\n",
      "making /home/ucsddse230/work/PA1_collinear_points/Data/Weather\n",
      "wget https://mas-dse-open.s3.amazonaws.com/Weather/by_state/NY.tgz -P /home/ucsddse230/work/PA1_collinear_points/Data/Weather \n",
      "--2019-10-28 16:39:19--  https://mas-dse-open.s3.amazonaws.com/Weather/by_state/NY.tgz\n",
      "Resolving mas-dse-open.s3.amazonaws.com (mas-dse-open.s3.amazonaws.com)... 52.218.236.3\n",
      "Connecting to mas-dse-open.s3.amazonaws.com (mas-dse-open.s3.amazonaws.com)|52.218.236.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66288146 (63M) [application/x-tar]\n",
      "Saving to: ‘/home/ucsddse230/work/PA1_collinear_points/Data/Weather/NY.tgz’\n",
      "\n",
      "NY.tgz              100%[===================>]  63.22M  5.84MB/s    in 11s     \n",
      "\n",
      "2019-10-28 16:39:32 (5.80 MB/s) - ‘/home/ucsddse230/work/PA1_collinear_points/Data/Weather/NY.tgz’ saved [66288146/66288146]\n",
      "\n",
      "-rwxr-xr-x 1 root root 64M Apr 19  2018 /home/ucsddse230/work/PA1_collinear_points/Data/Weather/NY.tgz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#trouver leurs fichier meteo:\\ndf_meteo = ...\\n#\\ndf_meteo.select('station','year','measurment').show(5)\\n#save pf_meteo dans un fichier parquet : \\ndata_dir = 'data'\\nfile_index = '1'\\nfilename = '%s/US_weather_%s.parquet'%(data_dir,file_index)\\n###!rm -rf $filename\\ndf_meteo.write.save(filename)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Meteorological dataframe :\n",
    "    =>\n",
    "\"\"\"\n",
    "from os.path import split,join,exists\n",
    "from os import mkdir,getcwd,remove\n",
    "from glob import glob\n",
    "\n",
    "def makedir(rep):\n",
    "    \"\"\"\n",
    "    Making a directory\n",
    "    Arguments : \n",
    "        => directory (type : String)\n",
    "    \"\"\"\n",
    "    if exists(rep):\n",
    "        print('directory',rep,'already exists')\n",
    "    else:\n",
    "        print('making',rep)\n",
    "        mkdir(rep)\n",
    "\n",
    "# create directory if needed\n",
    "\n",
    "notebook_dir=getcwd()\n",
    "print(notebook_dir)\n",
    "data_dir=join(notebook_dir,'data')\n",
    "print(data_dir)\n",
    "weather_dir=join(data_dir,'Weather')\n",
    "print(weather_dir)\n",
    "#creation repertoire weather\n",
    "makedir(weather_dir)\n",
    "\n",
    "file_index='NY'\n",
    "zip_file='%s.tgz'%(file_index) #the .csv extension is a mistake, this is a pickle file, not a csv file.\n",
    "old_files='%s/%s*'%(weather_dir,zip_file[:-3])\n",
    "for f in glob(old_files):\n",
    "    print('removing',f)\n",
    "    !rm -rf {f}\n",
    "\n",
    "command=\"wget https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s -P %s \"%(zip_file, weather_dir)\n",
    "print(command)\n",
    "!$command\n",
    "!ls -lh $weather_dir/$zip_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:40:25.533306Z",
     "start_time": "2019-10-28T16:40:21.650787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY.parquet/\n",
      "NY.parquet/_SUCCESS\n",
      "NY.parquet/part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#extracting the parquet file\n",
    "!tar zxvf {weather_dir}/{zip_file} -C {weather_dir}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:42:03.023089Z",
     "start_time": "2019-10-28T16:42:01.045528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ucsddse230/work/PA1_collinear_points/Data/Weather/NY.parquet\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#trouver leurs fichier meteo:\\ndf_meteo = ...\\n#\\ndf_meteo.select('station','year','measurment').show(5)\\n#save pf_meteo dans un fichier parquet : \\ndata_dir = 'data'\\nfile_index = '1'\\nfilename = '%s/US_weather_%s.parquet'%(data_dir,file_index)\\n###!rm -rf $filename\\ndf_meteo.write.save(filename)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_parquet = join(weather_dir, zip_file[:-3]+'parquet')\n",
    "print(weather_parquet)\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T16:47:21.329788Z",
     "start_time": "2019-10-28T16:47:16.251044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------+\n",
      "|    Station|Year|Measurement|\n",
      "+-----------+----+-----------+\n",
      "|USW00094704|1945|   PRCP_s20|\n",
      "|USW00094704|1946|   PRCP_s20|\n",
      "|USW00094704|1947|   PRCP_s20|\n",
      "|USW00094704|1948|   PRCP_s20|\n",
      "|USW00094704|1949|   PRCP_s20|\n",
      "+-----------+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "NY\n",
      "Fichier écrit :  /home/ucsddse230/work/PA1_collinear_points/Data /home/ucsddse230/work/PA1_collinear_points/Data/US_weather_NY.parquet NY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select('Station','Year','Measurement').show(5)\n",
    "print(file_index)\n",
    "\n",
    "#save pf_meteo dans un fichier parquet : \n",
    "\n",
    "filename = '%s/US_weather_%s.parquet'%(data_dir,file_index)\n",
    "#remove file (avoiding fucker overwrite)\n",
    "!rm -rf $filename\n",
    "df.write.save(filename)\n",
    "print('Fichier écrit : ',data_dir,filename,file_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
